{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Set, List, Callable, Tuple, TypedDict\n",
    "\n",
    "\n",
    "class Stats(TypedDict):\n",
    "    real_len: int\n",
    "    calculated_len: int\n",
    "    intersection_len: int\n",
    "    accuracy: float\n",
    "\n",
    "\n",
    "class Score(TypedDict):\n",
    "    sum_real: int\n",
    "    sum_intersection: int\n",
    "    accuracy: float\n",
    "\n",
    "\n",
    "def load_features(src: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(src)\n",
    "    df = df[[\"language\", \"numeric_id\"]]\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_features(df: pd.DataFrame, features_to_remove: List[str]) -> pd.DataFrame:\n",
    "    df = df[~df[\"language\"].isin(features_to_remove)]\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def keep_nodes(df: pd.DataFrame, number_of_nodes_to_keep: int) -> pd.DataFrame:\n",
    "    indexes_to_keep = np.random.choice(df.index, size=number_of_nodes_to_keep)\n",
    "    df = df.iloc[indexes_to_keep]\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_edges(src: str, features: pd.DataFrame) -> pd.DataFrame:\n",
    "    edges = pd.read_csv(src)\n",
    "\n",
    "    ids = features[\"numeric_id\"]\n",
    "\n",
    "    edges_1 = edges[edges[\"id_1\"].isin(ids)]\n",
    "    edges_2 = edges[edges[\"id_2\"].isin(ids)]\n",
    "    index = pd.Index.intersection(edges_1.index, edges_2.index)\n",
    "    edges = edges.iloc[index]\n",
    "    edges.reset_index(inplace=True, drop=True)\n",
    "    return edges\n",
    "\n",
    "\n",
    "def create_graph(edges: pd.DataFrame) -> nx.Graph:\n",
    "    G = nx.Graph()\n",
    "    for row in edges.iterrows():\n",
    "        G.add_edge(row[1][\"id_1\"], row[1][\"id_2\"])\n",
    "    return G\n",
    "\n",
    "\n",
    "def choose_largest_cc(graph: nx.Graph) -> nx.Graph:\n",
    "    largest_cc = max(nx.connected_components(graph), key=len)\n",
    "    graph = graph.subgraph(largest_cc)\n",
    "    return graph\n",
    "\n",
    "\n",
    "def get_real_communities(\n",
    "    grah: nx.Graph, features: pd.DataFrame\n",
    ") -> List[Tuple[str, Set[int]]]:\n",
    "    real_communities: Dict[str, Set[int]] = defaultdict(set)\n",
    "    for node in grah.nodes():\n",
    "        language = features[features[\"numeric_id\"] == node].iloc[0][\"language\"]\n",
    "        real_communities[language].add(node)\n",
    "    real_communities_list = [(k, v) for k, v in real_communities.items()]\n",
    "    return sorted(real_communities_list, key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "\n",
    "def set_weights(\n",
    "    graph: nx.Graph,\n",
    "    metric: Dict[int, float],\n",
    "    aggregate: Callable[[float, float], float],\n",
    "):\n",
    "    weights = {}\n",
    "    for edge in graph.edges():\n",
    "        n1, n2 = edge\n",
    "        m1, m2 = metric[n1], metric[n2]\n",
    "        weights[edge] = aggregate(m1, m2)\n",
    "\n",
    "    nx.set_edge_attributes(graph, weights, \"weights\")\n",
    "\n",
    "\n",
    "def calculate_communities(graph: nx.Graph) -> List[Set[int]]:\n",
    "    calculated_communities = list(\n",
    "        nx.algorithms.community.asyn_lpa_communities(graph, weight=\"weights\")\n",
    "    )\n",
    "    calculated_communities = sorted(calculated_communities, key=len, reverse=True)\n",
    "    return calculated_communities\n",
    "\n",
    "\n",
    "def calculate_mapping_stats(\n",
    "    real_communities: List[Tuple[str, Set[int]]], calculated_communities: List[Set[int]]\n",
    ") -> Dict[str, Stats]:\n",
    "    result: Dict[str, Stats] = {}\n",
    "\n",
    "    already_mapped = set()\n",
    "    for i in range(min(len(real_communities), len(calculated_communities))):\n",
    "        calculated = calculated_communities[i]\n",
    "\n",
    "        best = {\"language\": \"\", \"real_len\": -1, \"intersection_len\": -1}\n",
    "\n",
    "        for language, real in real_communities:\n",
    "            if language in already_mapped:\n",
    "                continue\n",
    "\n",
    "            intersection_len = len(real.intersection(calculated))\n",
    "\n",
    "            if intersection_len > best[\"intersection_len\"]:\n",
    "                best = {\n",
    "                    \"language\": language,\n",
    "                    \"real_len\": len(real),\n",
    "                    \"intersection_len\": intersection_len,\n",
    "                }\n",
    "\n",
    "        result[best[\"language\"]] = {\n",
    "            \"real_len\": best[\"real_len\"],\n",
    "            \"calculated_len\": len(calculated),\n",
    "            \"intersection_len\": best[\"intersection_len\"],\n",
    "            \"accuracy\": best[\"intersection_len\"] / best[\"real_len\"],\n",
    "        }\n",
    "        already_mapped.add(best[\"language\"])\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def calculate_score(mapping_stats: Dict[str, Stats]) -> Score:\n",
    "    sum_real = 0\n",
    "    sum_intersection = 0\n",
    "    for stats in mapping_stats.values():\n",
    "        sum_real += stats[\"real_len\"]\n",
    "        sum_intersection += stats[\"intersection_len\"]\n",
    "    return {\n",
    "        \"sum_real\": sum_real,\n",
    "        \"sum_intersection\": sum_intersection,\n",
    "        \"accuracy\": sum_intersection / sum_real,\n",
    "    }\n",
    "\n",
    "\n",
    "def eval(G: nx.Graph, real_communities: List[Tuple[str, Set[int]]], iterations: int):\n",
    "    scores = []\n",
    "    for _ in range(iterations):\n",
    "        calculated_communities: List[Set[int]] = calculate_communities(G)\n",
    "        mapping_stats = calculate_mapping_stats(\n",
    "            real_communities, calculated_communities\n",
    "        )\n",
    "        score = calculate_score(mapping_stats)[\"accuracy\"]\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 168114 nodes\n"
     ]
    }
   ],
   "source": [
    "features = load_features(\"./datasets/twitch/large_twitch_features.csv\")\n",
    "print(\"Loaded\", len(features.index), \"nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 166685 nodes after removing ['OTHER']\n"
     ]
    }
   ],
   "source": [
    "features_to_remove = [\"OTHER\"]\n",
    "features = remove_features(features, features_to_remove)\n",
    "print(\"Kept\", len(features.index), \"nodes after removing\", features_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 42274 nodes after removing ['EN']\n"
     ]
    }
   ],
   "source": [
    "# number_of_nodes_to_keep = 1_000\n",
    "# features = keep_nodes(features, number_of_nodes_to_keep)\n",
    "# print(\"Kept\", len(features.index), \"nodes\")\n",
    "\n",
    "features_to_remove = [\"EN\"]\n",
    "features = remove_features(features, features_to_remove)\n",
    "print(\"Kept\", len(features.index), \"nodes after removing\", features_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 657915 edges\n"
     ]
    }
   ],
   "source": [
    "edges = load_edges(\"./datasets/twitch/large_twitch_edges.csv\", features)\n",
    "print(\"Loaded\", len(edges.index), \"edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created graph with 41309 nodes and 657915 edges\n"
     ]
    }
   ],
   "source": [
    "G = create_graph(edges)\n",
    "\n",
    "print(\n",
    "    \"Created graph with\", G.number_of_nodes(), \"nodes and\", G.number_of_edges(), \"edges\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chose largest connected component with 41265 nodes and 657892 edges\n"
     ]
    }
   ],
   "source": [
    "G = choose_largest_cc(G)\n",
    "print(\n",
    "    \"Chose largest connected component with\",\n",
    "    G.number_of_nodes(),\n",
    "    \"nodes and\",\n",
    "    G.number_of_edges(),\n",
    "    \"edges\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  19 communites with the folowing counts:\n",
      "- DE 9270\n",
      "- FR 6734\n",
      "- ES 5562\n",
      "- RU 4694\n",
      "- ZH 2794\n",
      "- PT 2460\n",
      "- JA 1262\n",
      "- IT 1210\n",
      "- KO 1186\n",
      "- PL 906\n",
      "- SV 781\n",
      "- TR 761\n",
      "- NL 647\n",
      "- TH 631\n",
      "- FI 623\n",
      "- CS 569\n",
      "- DA 472\n",
      "- HU 414\n",
      "- NO 289\n"
     ]
    }
   ],
   "source": [
    "real_communities = get_real_communities(G, features)\n",
    "\n",
    "print(\"There are \", len(real_communities), \"communites with the folowing counts:\")\n",
    "for language, community in real_communities:\n",
    "    print(\"-\", language, len(community))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = {}\n",
    "# for node, degree in G.degree():\n",
    "#     metric[node] = degree\n",
    "# set_weights(G, metric, max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculated_communities: List[Set[int]] = calculate_communities(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 communities with the folowing counts\n",
      "- 18\n",
      "- 10\n",
      "- 9\n",
      "- 7\n",
      "- 7\n",
      "- 5\n",
      "- 4\n",
      "- 3\n",
      "- 3\n",
      "- 3\n",
      "- 3\n",
      "- 2\n",
      "- 2\n",
      "- 2\n",
      "- 2\n"
     ]
    }
   ],
   "source": [
    "# print(\"Found\", len(calculated_communities), \"communities with the folowing counts\")\n",
    "# for community in calculated_communities:\n",
    "#     print(\"-\", len(community))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- EN {'real_len': 76, 'calculated_len': 18, 'intersection_len': 18, 'accuracy': 0.23684210526315788}\n",
      "- DE {'real_len': 3, 'calculated_len': 10, 'intersection_len': 0, 'accuracy': 0.0}\n",
      "- NO {'real_len': 1, 'calculated_len': 9, 'intersection_len': 0, 'accuracy': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# mapping_stats = calculate_mapping_stats(real_communities, calculated_communities)\n",
    "# for language, stats in mapping_stats.items():\n",
    "#     print(\"-\", language, stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sum_real': 80, 'sum_intersection': 18, 'accuracy': 0.225}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate_score(mapping_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = eval(G, real_communities, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1875\n",
      "0.3875\n",
      "0.28874999999999995\n",
      "0.07125000000000001\n"
     ]
    }
   ],
   "source": [
    "# print(np.min(scores))\n",
    "# print(np.max(scores))\n",
    "# print(np.mean(scores))\n",
    "# print(np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_unitary(graph: nx.Graph) -> Dict[int, int]:\n",
    "    metric = {}\n",
    "    for node in graph.nodes():\n",
    "        metric[node] = 1\n",
    "    return metric\n",
    "\n",
    "\n",
    "def metric_degree(graph: nx.Graph) -> Dict[int, int]:\n",
    "    metric = {}\n",
    "    for node, degree in graph.degree():\n",
    "        metric[node] = degree\n",
    "    return metric\n",
    "\n",
    "\n",
    "def metric_degree_centrality(graph: nx.Graph) -> Dict[int, int]:\n",
    "    return nx.degree_centrality(graph)\n",
    "\n",
    "\n",
    "def metric_eigenvector_centrality(graph: nx.Graph) -> Dict[int, int]:\n",
    "    return nx.eigenvector_centrality(graph)\n",
    "\n",
    "\n",
    "def metric_pagerank(graph: nx.Graph) -> Dict[int, int]:\n",
    "    return nx.pagerank(graph)\n",
    "\n",
    "\n",
    "def metric_clustering(graph: nx.Graph) -> Dict[int, int]:\n",
    "    return nx.clustering(graph)\n",
    "\n",
    "\n",
    "def metric_closeness_centrality(graph: nx.Graph) -> Dict[int, int]:\n",
    "    return nx.closeness_centrality(graph)\n",
    "\n",
    "\n",
    "def metric_betweeness_centrality(graph: nx.Graph) -> Dict[int, int]:\n",
    "    return nx.betweenness_centrality(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    (\"unitary\", metric_unitary, max),\n",
    "    (\"degree max\", metric_degree, max),\n",
    "    (\"degree_centrality max\", metric_degree_centrality, max),\n",
    "    (\"eigenvector_centrality max\", metric_eigenvector_centrality, max),\n",
    "    (\"pagerank max\", metric_pagerank, max),\n",
    "    (\"clustering max\", metric_clustering, max),\n",
    "    (\"closeness_centrality max\", metric_closeness_centrality, max),\n",
    "    (\"betweeness_centrality max\", metric_betweeness_centrality, max),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating unitary\n",
      "Generated weights\n",
      "Evaluating...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "for test in tests:\n",
    "    result = {}\n",
    "    print(\"Evaluating\", test[0])\n",
    "    set_weights(G, test[1](G), test[2])\n",
    "    print(\"Generated weights\")\n",
    "    print(\"Evaluating...\")\n",
    "    scores = eval(G, real_communities, 10)\n",
    "    result[\"mean\"] = np.mean(scores)\n",
    "    result[\"std\"] = np.std(scores)\n",
    "    result[\"min\"] = np.min(scores)\n",
    "    result[\"max\"] = np.max(scores)\n",
    "    result[\"scores\"] = scores\n",
    "    print(\"Saving\")\n",
    "    with open(f\"eval/{test[0]}.json\", \"w\") as f:\n",
    "        json.dump(result, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
