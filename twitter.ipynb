{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des différents packages\n",
    "\n",
    "import networkx as nx\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List\n",
    "import matplotlib\n",
    "import json\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrer les caractéristiques dans stats_twitter.json\n",
    "\n",
    "def save_stats(stats):\n",
    "    with open(\"stats_twitter.json\", \"w\") as f:\n",
    "        json.dump(stats, f, sort_keys=True, indent=4)\n",
    "\n",
    "def load_stats() -> Dict:\n",
    "    with open(\"stats_twitter.json\", \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def print_stats(stats):\n",
    "    print(json.dumps(stats, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modification sur le style du graph\n",
    "\n",
    "def highlight_nodes(graph: nx.Graph, nodes: List = []):\n",
    "    reset_colors(graph)\n",
    "    for node in nodes:\n",
    "        graph.nodes[node][\"viz\"][\"color\"] = {'r': 255, 'g': 0, 'b': 0, 'a': 1}\n",
    "\n",
    "def highlight_nodes_importance(graph: nx.Graph, node_importance : Dict = {}):\n",
    "    reset_colors(graph)\n",
    "    min = np.min(list(node_importance.values()))\n",
    "    max = np.max(list(node_importance.values()))\n",
    "    norm = matplotlib.colors.Normalize(vmin=min, vmax=max)\n",
    "    for node, importance in node_importance.items():\n",
    "        g = int(255 * norm(importance))\n",
    "        r = 255 - g\n",
    "        graph.nodes[node][\"viz\"][\"color\"] = {'r': r, 'g': g, 'b': 0, 'a': 1}\n",
    "\n",
    "def highlight_nodes_communities(graph: nx.Graph, communities: List[List] = []):\n",
    "    reset_colors(graph)\n",
    "    cmap: LinearSegmentedColormap = plt.get_cmap('hsv')\n",
    "    for i, community in enumerate(communities):\n",
    "        (r, g, b, a) = cmap(i / len(communities))\n",
    "        r, g, b, a = int(r * 255), int(g * 255), int(b * 255), a\n",
    "        for node in community:\n",
    "            graph.nodes[node][\"viz\"][\"color\"] = {'r': r, 'g': g, 'b': b, 'a': a}\n",
    "\n",
    "\n",
    "def reset_colors(graph: nx.Graph):\n",
    "    for node in graph:\n",
    "        if(not \"viz\" in graph.nodes[node]):\n",
    "            graph.nodes[node][\"viz\"] = {}\n",
    "        graph.nodes[node][\"viz\"][\"color\"] = {'r': 173, 'g': 216, 'b': 230, 'a': 1}\n",
    "\n",
    "\n",
    "def circular_layout(graph: nx.Graph):\n",
    "    pos = nx.circular_layout(graph, scale=4*graph.number_of_nodes())\n",
    "    for node, position in pos.items():\n",
    "        if(not \"viz\" in graph.nodes[node]):\n",
    "            graph.nodes[node][\"viz\"] = {}\n",
    "        graph.nodes[node][\"viz\"][\"position\"] = {\"x\": position[0], \"y\": position[1], \"z\": 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'datasets/twitter/twitter_combined.txt'\n",
    "STATS = load_stats()\n",
    "\n",
    "G = nx.read_edgelist(path=DATASET_PATH, create_using=nx.DiGraph(), nodetype=int)\n",
    "\n",
    "circular_layout(G)\n",
    "reset_colors(G)\n",
    "\n",
    "nx.write_gexf(G, \"graphs/twitter/graph.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATS[\"number_of_nodes\"] =  G.number_of_nodes()\n",
    "save_stats(STATS)\n",
    "\n",
    "STATS[\"number_of_edges\"] =  G.number_of_edges()\n",
    "save_stats(STATS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étude de la centralité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la centralité de degré pour les nœuds échantillonnés\n",
    "\n",
    "STATS[\"degree_centrality\"] = nx.degree_centrality(G)\n",
    "save_stats(STATS)\n",
    "\n",
    "highlight_nodes_importance(G, STATS[\"degree_centrality\"])\n",
    "nx.write_gexf(G, \"graphs/twitter/degree_centrality.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la centralité de vecteur propre\n",
    "\n",
    "STATS[\"eigenvector_centrality\"] = nx.eigenvector_centrality(G, max_iter=500, tol=1e-06)\n",
    "save_stats(STATS)\n",
    "\n",
    "highlight_nodes_importance(G, STATS[\"eigenvector_centrality\"])\n",
    "nx.write_gexf(G, \"graphs/twitter/eigenvector_centrality.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la centralité avec pagerank\n",
    "\n",
    "STATS[\"pagerank\"] = nx.pagerank(G)\n",
    "save_stats(STATS)\n",
    "\n",
    "highlight_nodes_importance(G, STATS[\"pagerank\"])\n",
    "nx.write_gexf(G, \"graphs/twitter/pagerank.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul la centralité avec closeness_centrality\n",
    "\n",
    "STATS[\"closeness_centrality\"] = nx.closeness_centrality(G)\n",
    "save_stats(STATS)\n",
    "highlight_nodes_importance(G, STATS[\"closeness_centrality\"])\n",
    "nx.write_gexf(G, \"graphs/twitter/closeness_centrality.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul la centralité avec betweenness_centrality\n",
    "\n",
    "STATS[\"betweenness_centrality\"] = nx.betweenness_centrality(G)\n",
    "save_stats(STATS)\n",
    "highlight_nodes_importance(G, STATS[\"betweenness_centrality\"])\n",
    "nx.write_gexf(G, \"graphs/twitter/betweenness_centrality.gexf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Treshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle Linear Threshold pour la diffusion des informations\n",
    "\n",
    "def linear_threshold_simulation(G, seed_users, threshold):\n",
    "    active_users = seed_users.copy()\n",
    "    new_active_users = seed_users.copy()\n",
    "\n",
    "    while new_active_users:\n",
    "        new_active_users_temp = []\n",
    "        for user in new_active_users:\n",
    "            neighbors = G.neighbors(user)\n",
    "            for neighbor in neighbors:\n",
    "                if neighbor not in active_users:\n",
    "                    active_neighbors = [n for n in G.neighbors(neighbor) if n in active_users]\n",
    "                    if len(active_neighbors) / G.degree(neighbor) >= threshold:\n",
    "                        new_active_users_temp.append(neighbor)\n",
    "                        active_users.append(neighbor)\n",
    "        new_active_users = new_active_users_temp\n",
    "\n",
    "    return active_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Influence Maximization\n",
    "\n",
    "def influence_maximization_threshold(G, k, threshold, num_simulations=100):\n",
    "    seed_users = []\n",
    "\n",
    "    for _ in range(k):\n",
    "        max_influence = -1\n",
    "        best_candidate = None\n",
    "\n",
    "        for candidate in G.nodes():\n",
    "            if candidate not in seed_users:\n",
    "                total_active_users = 0\n",
    "                for _ in range(num_simulations):\n",
    "                    active_users = linear_threshold_simulation(G, seed_users + [candidate], threshold)\n",
    "                    total_active_users += len(active_users)\n",
    "                average_active_users = total_active_users / num_simulations\n",
    "\n",
    "                if average_active_users > max_influence:\n",
    "                    max_influence = average_active_users\n",
    "                    best_candidate = candidate\n",
    "\n",
    "        seed_users.append(best_candidate)\n",
    "\n",
    "    return seed_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the top K seed users based on a centrality measure\n",
    "\n",
    "K = 10\n",
    "top_degree_users = sorted(STATS[\"degree_centrality\"], key=STATS[\"degree_centrality\"].get, reverse=True)[:K]\n",
    "top_closeness_users = sorted(STATS[\"closeness_centrality\"], key=STATS[\"closeness_centrality\"].get, reverse=True)[:K]\n",
    "top_betweenness_users = sorted(STATS[\"betweenness_centrality\"], key=STATS[\"betweenness_centrality\"].get, reverse=True)[:K]\n",
    "top_eigenvector_users = sorted(STATS[\"eigenvector_centrality\"], key=STATS[\"eigenvector_centrality\"].get, reverse=True)[:K]\n",
    "top_pagerank_users = sorted(STATS[\"pagerank\"], key=STATS[\"pagerank\"].get, reverse=True)[:K]\n",
    "\n",
    "\n",
    "seed_user_sets = {\n",
    "    'degree': top_degree_users,\n",
    "    'closeness': top_closeness_users,\n",
    "    'betweenness': top_betweenness_users,\n",
    "    'eigenvector': top_eigenvector_users,\n",
    "    'pagerank': top_pagerank_users\n",
    "}\n",
    "\n",
    "# Test different threshold values\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "results = {}\n",
    "for centrality, seed_users in seed_user_sets.items():\n",
    "    results[centrality] = {}\n",
    "    for threshold in thresholds:\n",
    "        # Run multiple simulations and take the average\n",
    "        num_simulations = 10\n",
    "        total_active_users = 0\n",
    "        for _ in range(num_simulations):\n",
    "            active_users = linear_threshold_simulation(G, seed_users, threshold)\n",
    "            total_active_users += len(active_users)\n",
    "        \n",
    "        average_active_users = total_active_users / num_simulations\n",
    "        results[centrality][threshold] = average_active_users\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent_cascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle Independent_cascade pour la diffusion des informations\n",
    "\n",
    "def independent_cascade_simulation(G, seed_users, p):\n",
    "    active_users = seed_users.copy()\n",
    "    new_active_users = seed_users.copy()\n",
    "\n",
    "    while new_active_users:\n",
    "        new_active_users_temp = []\n",
    "        for user in new_active_users:\n",
    "            neighbors = G.neighbors(user)\n",
    "            for neighbor in neighbors:\n",
    "                if neighbor not in active_users:\n",
    "                    if random.random() < p:\n",
    "                        new_active_users_temp.append(neighbor)\n",
    "                        active_users.append(neighbor)\n",
    "        new_active_users = new_active_users_temp\n",
    "\n",
    "    return active_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Influence Maximization\n",
    "\n",
    "def influence_maximization_cascade(G, k, p, num_simulations=100):\n",
    "    seed_users = []\n",
    "\n",
    "    for _ in range(k):\n",
    "        max_influence = -1\n",
    "        best_candidate = None\n",
    "\n",
    "        for candidate in G.nodes():\n",
    "            if candidate not in seed_users:\n",
    "                total_active_users = 0\n",
    "                for _ in range(num_simulations):\n",
    "                    active_users = independent_cascade_simulation(G, seed_users + [candidate], p)\n",
    "                    total_active_users += len(active_users)\n",
    "                average_active_users = total_active_users / num_simulations\n",
    "\n",
    "                if average_active_users > max_influence:\n",
    "                    max_influence = average_active_users\n",
    "                    best_candidate = candidate\n",
    "\n",
    "        seed_users.append(best_candidate)\n",
    "\n",
    "    return seed_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the top K seed users based on each centrality measure\n",
    "\n",
    "K = 10\n",
    "top_degree_users = sorted(STATS[\"degree_centrality\"], key=STATS[\"degree_centrality\"].get, reverse=True)[:K]\n",
    "top_closeness_users = sorted(STATS[\"closeness_centrality\"], key=STATS[\"closeness_centrality\"].get, reverse=True)[:K]\n",
    "top_betweenness_users = sorted(STATS[\"betweenness_centrality\"], key=STATS[\"betweenness_centrality\"].get, reverse=True)[:K]\n",
    "top_eigenvector_users = sorted(STATS[\"eigenvector_centrality\"], key=STATS[\"eigenvector_centrality\"].get, reverse=True)[:K]\n",
    "top_pagerank_users = sorted(STATS[\"pagerank\"], key=STATS[\"pagerank\"].get, reverse=True)[:K]\n",
    "\n",
    "\n",
    "seed_user_sets = {\n",
    "    'degree': top_degree_users,\n",
    "    'closeness': top_closeness_users,\n",
    "    'betweenness': top_betweenness_users,\n",
    "    'eigenvector': top_eigenvector_users,\n",
    "    'pagerank': top_pagerank_users\n",
    "}\n",
    "\n",
    "# Test different probabilities for the Independent Cascade Model\n",
    "probabilities = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "results = {}\n",
    "for centrality, seed_users in seed_user_sets.items():\n",
    "    results[centrality] = {}\n",
    "    for p in probabilities:\n",
    "        # Run multiple simulations and take the average\n",
    "        num_simulations = 10\n",
    "        total_active_users = 0\n",
    "        for _ in range(num_simulations):\n",
    "            active_users = independent_cascade_simulation(G, seed_users, p)\n",
    "            total_active_users += len(active_users)\n",
    "        \n",
    "        average_active_users = total_active_users / num_simulations\n",
    "        results[centrality][p] = average_active_users\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
