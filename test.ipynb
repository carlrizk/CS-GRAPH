{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Set, List, Callable, Tuple, TypedDict\n",
    "from utils import *\n",
    "import json\n",
    "\n",
    "\n",
    "class Stats(TypedDict):\n",
    "    real_len: int\n",
    "    calculated_len: int\n",
    "    intersection_len: int\n",
    "    accuracy: float\n",
    "\n",
    "\n",
    "class Score(TypedDict):\n",
    "    sum_real: int\n",
    "    sum_intersection: int\n",
    "    accuracy: float\n",
    "\n",
    "\n",
    "def load_features(src: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(src)\n",
    "    df = df[[\"language\", \"numeric_id\"]]\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_features(df: pd.DataFrame, features_to_remove: List[str]) -> pd.DataFrame:\n",
    "    df = df[~df[\"language\"].isin(features_to_remove)]\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def keep_nodes(df: pd.DataFrame, number_of_nodes_to_keep: int) -> pd.DataFrame:\n",
    "    indexes_to_keep = np.random.choice(df.index, size=number_of_nodes_to_keep)\n",
    "    df = df.iloc[indexes_to_keep]\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_edges(src: str, features: pd.DataFrame) -> pd.DataFrame:\n",
    "    edges = pd.read_csv(src)\n",
    "\n",
    "    ids = features[\"numeric_id\"]\n",
    "\n",
    "    edges_1 = edges[edges[\"id_1\"].isin(ids)]\n",
    "    edges_2 = edges[edges[\"id_2\"].isin(ids)]\n",
    "    index = pd.Index.intersection(edges_1.index, edges_2.index)\n",
    "    edges = edges.iloc[index]\n",
    "    edges.reset_index(inplace=True, drop=True)\n",
    "    return edges\n",
    "\n",
    "\n",
    "def create_graph(edges: pd.DataFrame) -> nx.Graph:\n",
    "    G = nx.Graph()\n",
    "    for row in edges.iterrows():\n",
    "        G.add_edge(row[1][\"id_1\"], row[1][\"id_2\"])\n",
    "    return G\n",
    "\n",
    "\n",
    "def choose_largest_cc(graph: nx.Graph) -> nx.Graph:\n",
    "    largest_cc = max(nx.connected_components(graph), key=len)\n",
    "    graph = graph.subgraph(largest_cc)\n",
    "    return graph\n",
    "\n",
    "\n",
    "def get_real_communities(\n",
    "    grah: nx.Graph, features: pd.DataFrame\n",
    ") -> List[Tuple[str, Set[int]]]:\n",
    "    real_communities: Dict[str, Set[int]] = defaultdict(set)\n",
    "    for node in grah.nodes():\n",
    "        language = features[features[\"numeric_id\"] == node].iloc[0][\"language\"]\n",
    "        real_communities[language].add(node)\n",
    "    real_communities_list = [(k, v) for k, v in real_communities.items()]\n",
    "    return sorted(real_communities_list, key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "\n",
    "def set_weights(\n",
    "    graph: nx.Graph,\n",
    "    metric: Dict[int, float],\n",
    "    aggregate: Callable[[List[float]], float],\n",
    "):\n",
    "    weights = {}\n",
    "    for edge in graph.edges():\n",
    "        n1, n2 = edge\n",
    "        m1, m2 = metric[n1], metric[n2]\n",
    "        weights[edge] = aggregate([m1, m2])\n",
    "\n",
    "    nx.set_edge_attributes(graph, weights, \"weights\")\n",
    "\n",
    "\n",
    "def calculate_communities(graph: nx.Graph) -> List[Set[int]]:\n",
    "    calculated_communities = list(\n",
    "        nx.algorithms.community.asyn_lpa_communities(graph, weight=\"weights\")\n",
    "    )\n",
    "    calculated_communities = sorted(calculated_communities, key=len, reverse=True)\n",
    "    return calculated_communities\n",
    "\n",
    "\n",
    "def calculate_mapping_stats(\n",
    "    real_communities: List[Tuple[str, Set[int]]], calculated_communities: List[Set[int]]\n",
    ") -> Dict[str, Stats]:\n",
    "    result: Dict[str, Stats] = {}\n",
    "\n",
    "    already_mapped = set()\n",
    "    for i in range(min(len(real_communities), len(calculated_communities))):\n",
    "        calculated = calculated_communities[i]\n",
    "\n",
    "        best = {\n",
    "            \"language\": \"\",\n",
    "            \"real_len\": -1,\n",
    "            \"intersection_len\": -1,\n",
    "            \"accuracy\": -1,\n",
    "        }\n",
    "\n",
    "        for language, real in real_communities:\n",
    "            if language in already_mapped:\n",
    "                continue\n",
    "\n",
    "            intersection_len = len(real.intersection(calculated))\n",
    "            accuracy = intersection_len / (\n",
    "                len(real) + len(calculated) - intersection_len\n",
    "            )\n",
    "\n",
    "            if accuracy > best[\"accuracy\"]:\n",
    "                best = {\n",
    "                    \"language\": language,\n",
    "                    \"real_len\": len(real),\n",
    "                    \"intersection_len\": intersection_len,\n",
    "                    \"accuracy\": accuracy,\n",
    "                }\n",
    "\n",
    "        result[best[\"language\"]] = {\n",
    "            \"real_len\": best[\"real_len\"],\n",
    "            \"calculated_len\": len(calculated),\n",
    "            \"intersection_len\": best[\"intersection_len\"],\n",
    "            \"accuracy\": best[\"accuracy\"],\n",
    "        }\n",
    "        already_mapped.add(best[\"language\"])\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def calculate_score(mapping_stats: Dict[str, Stats]) -> Score:\n",
    "    sum_real = 0\n",
    "    sum_intersection = 0\n",
    "    sum_calculated = 0\n",
    "    for stats in mapping_stats.values():\n",
    "        sum_real += stats[\"real_len\"]\n",
    "        sum_intersection += stats[\"intersection_len\"]\n",
    "        sum_calculated += stats[\"calculated_len\"]\n",
    "    return {\n",
    "        \"sum_real\": sum_real,\n",
    "        \"sum_intersection\": sum_intersection,\n",
    "        \"sum_calculated\": sum_calculated,\n",
    "        \"accuracy\": sum_intersection / max(sum_real, sum_calculated),\n",
    "    }\n",
    "\n",
    "\n",
    "def eval(G: nx.Graph, real_communities: List[Tuple[str, Set[int]]], iterations: int):\n",
    "    scores = []\n",
    "    for _ in range(iterations):\n",
    "        calculated_communities: List[Set[int]] = calculate_communities(G)\n",
    "        mapping_stats = calculate_mapping_stats(\n",
    "            real_communities, calculated_communities\n",
    "        )\n",
    "        score = calculate_score(mapping_stats)[\"accuracy\"]\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 168114 nodes\n"
     ]
    }
   ],
   "source": [
    "features = load_features(\"./datasets/twitch/large_twitch_features.csv\")\n",
    "print(\"Loaded\", len(features.index), \"nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 166685 nodes after removing ['OTHER']\n"
     ]
    }
   ],
   "source": [
    "features_to_remove = [\"OTHER\"]\n",
    "features = remove_features(features, features_to_remove)\n",
    "print(\"Kept\", len(features.index), \"nodes after removing\", features_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 42274 nodes after removing ['EN']\n"
     ]
    }
   ],
   "source": [
    "features_to_remove = [\"EN\"]\n",
    "features = remove_features(features, features_to_remove)\n",
    "print(\"Kept\", len(features.index), \"nodes after removing\", features_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 657915 edges\n"
     ]
    }
   ],
   "source": [
    "edges = load_edges(\"./datasets/twitch/large_twitch_edges.csv\", features)\n",
    "print(\"Loaded\", len(edges.index), \"edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created graph with 41309 nodes and 657915 edges\n"
     ]
    }
   ],
   "source": [
    "G = create_graph(edges)\n",
    "\n",
    "print(\n",
    "    \"Created graph with\", G.number_of_nodes(), \"nodes and\", G.number_of_edges(), \"edges\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chose largest connected component with 41265 nodes and 657892 edges\n"
     ]
    }
   ],
   "source": [
    "G = choose_largest_cc(G)\n",
    "print(\n",
    "    \"Chose largest connected component with\",\n",
    "    G.number_of_nodes(),\n",
    "    \"nodes and\",\n",
    "    G.number_of_edges(),\n",
    "    \"edges\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  19 communites with the folowing counts:\n",
      "- DE 9270\n",
      "- FR 6734\n",
      "- ES 5562\n",
      "- RU 4694\n",
      "- ZH 2794\n",
      "- PT 2460\n",
      "- JA 1262\n",
      "- IT 1210\n",
      "- KO 1186\n",
      "- PL 906\n",
      "- SV 781\n",
      "- TR 761\n",
      "- NL 647\n",
      "- TH 631\n",
      "- FI 623\n",
      "- CS 569\n",
      "- DA 472\n",
      "- HU 414\n",
      "- NO 289\n"
     ]
    }
   ],
   "source": [
    "real_communities = get_real_communities(G, features)\n",
    "\n",
    "print(\"There are \", len(real_communities), \"communites with the folowing counts:\")\n",
    "for language, community in real_communities:\n",
    "    print(\"-\", language, len(community))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO = real_communities[-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = [node for node in G.nodes() if node not in NO]\n",
    "\n",
    "G_NO = G.copy()\n",
    "G_NO.remove_nodes_from(to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289 1265\n"
     ]
    }
   ],
   "source": [
    "print(G_NO.number_of_nodes(), G_NO.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n"
     ]
    }
   ],
   "source": [
    "reset_colors(G_NO)\n",
    "\n",
    "highlight = set()\n",
    "for node in G_NO.nodes():\n",
    "    neighbors = G.neighbors(node)\n",
    "    if any([nei not in NO for nei in neighbors]):\n",
    "        highlight.add(node)\n",
    "print(len(highlight))\n",
    "\n",
    "highlight_nodes(G_NO, highlight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "externals = set()\n",
    "\n",
    "for n1, n2 in G.edges:\n",
    "    is_1 = n1 in NO\n",
    "    is_2 = n2 in NO\n",
    "    if is_1 and is_2:\n",
    "        continue\n",
    "    if is_1:\n",
    "        internal = n1\n",
    "        external = n2\n",
    "    if is_2:\n",
    "        internal = n2\n",
    "        external = n1\n",
    "    if is_1 or is_2:\n",
    "        externals.add(external)\n",
    "        G_NO.add_edge(internal, external)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = {}\n",
    "for node, degree in G.degree(G_NO.nodes()):\n",
    "    if degree not in deg:\n",
    "        deg[degree] = 0\n",
    "    deg[degree] += 1\n",
    "\n",
    "for node in G_NO.nodes():\n",
    "    degree = G_NO.degree(node)\n",
    "    nbre_for_degree = deg[degree]\n",
    "    if \"viz\" not in G_NO.nodes[node]:\n",
    "        G_NO.nodes[node][\"viz\"] = {\"position\": {}}\n",
    "    G_NO.nodes[node][\"viz\"][\"position\"] = {\n",
    "        \"x\": 20 * degree,\n",
    "        \"y\": 20 * nbre_for_degree,\n",
    "        \"z\": 0.0,\n",
    "    }\n",
    "    deg[degree] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(G_NO, \"graphs/twitch/NO.gexf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
