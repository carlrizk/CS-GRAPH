{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import networkx as nx\n",
    "import random\n",
    "import pandas as pd\n",
    "from utils import (\n",
    "    load_stats,\n",
    "    circular_layout,\n",
    "    reset_colors,\n",
    "    save_stats,\n",
    "    highlight_nodes_importance,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs_sampling(G, start_node, num_nodes_to_sample):\n",
    "    visited = set()\n",
    "    queue = [start_node]\n",
    "\n",
    "    while queue and len(visited) < num_nodes_to_sample:\n",
    "        node = queue.pop(0)\n",
    "        if node not in visited:\n",
    "            visited.add(node)\n",
    "            neighbors = list(G.neighbors(node))\n",
    "            random.shuffle(neighbors)\n",
    "            queue.extend(neighbors)\n",
    "\n",
    "    subgraph = G.subgraph(visited)\n",
    "    return subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"dataset/twitter_combined.txt\"\n",
    "STATS = load_stats()\n",
    "\n",
    "G = nx.read_edgelist(path=DATASET_PATH, create_using=nx.DiGraph(), nodetype=int)\n",
    "\n",
    "\n",
    "centrality = nx.degree_centrality(G)\n",
    "# Trouvez le noeud avec la plus grande centralité de degré\n",
    "max_degree_node = max(centrality, key=centrality.get)\n",
    "# On garde une partie du graph en partant de ce noeud\n",
    "G = bfs_sampling(G, max_degree_node, 3000)\n",
    "\n",
    "\n",
    "circular_layout(G)\n",
    "reset_colors(G)\n",
    "\n",
    "nx.write_gexf(G, \"graphs/graph.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATS[\"number_of_nodes\"] = G.number_of_nodes()\n",
    "save_stats(STATS)\n",
    "\n",
    "STATS[\"number_of_edges\"] = G.number_of_edges()\n",
    "save_stats(STATS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étude de la centralité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la centralité de degré pour les nœuds échantillonnés\n",
    "\n",
    "STATS[\"degree_centrality\"] = nx.degree_centrality(G)\n",
    "save_stats(STATS)\n",
    "\n",
    "highlight_nodes_importance(G, STATS[\"degree_centrality\"])\n",
    "nx.write_gexf(G, \"graphs/degree_centrality.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la centralité avec pagerank\n",
    "\n",
    "STATS[\"pagerank\"] = nx.pagerank(G)\n",
    "save_stats(STATS)\n",
    "\n",
    "highlight_nodes_importance(G, STATS[\"pagerank\"])\n",
    "nx.write_gexf(G, \"graphs/pagerank.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul la centralité avec closeness_centrality\n",
    "\n",
    "STATS[\"closeness_centrality\"] = nx.closeness_centrality(G)\n",
    "save_stats(STATS)\n",
    "highlight_nodes_importance(G, STATS[\"closeness_centrality\"])\n",
    "nx.write_gexf(G, \"graphs/closeness_centrality.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul la centralité avec betweenness_centrality\n",
    "\n",
    "STATS[\"betweenness_centrality\"] = nx.betweenness_centrality(G)\n",
    "save_stats(STATS)\n",
    "highlight_nodes_importance(G, STATS[\"betweenness_centrality\"])\n",
    "nx.write_gexf(G, \"graphs/betweenness_centrality.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methodes = {\n",
    "    \"degree_centrality\": STATS[\"degree_centrality\"],\n",
    "    \"closeness_centrality\": STATS[\"closeness_centrality\"],\n",
    "    \"betweenness_centrality\": STATS[\"betweenness_centrality\"],\n",
    "    \"pagerank\": STATS[\"pagerank\"],\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independant cascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle Independent_cascade pour la diffusion des informations. On prend w(u,v) = 1/dv-\n",
    "\n",
    "\n",
    "def independent_cascade_simulation(G, seed_users):\n",
    "    active_users = seed_users.copy()\n",
    "    new_active_users = seed_users.copy()\n",
    "\n",
    "    while new_active_users:\n",
    "        newly_activated_users = []\n",
    "\n",
    "        for active_user in new_active_users:\n",
    "            for neighbor in G.neighbors(active_user):\n",
    "                if neighbor not in active_users:\n",
    "                    indegree = G.in_degree(neighbor)\n",
    "                    p = 1 / indegree\n",
    "\n",
    "                    if random.random() <= p:\n",
    "                        newly_activated_users.append(neighbor)\n",
    "                        active_users.append(neighbor)\n",
    "\n",
    "        new_active_users = newly_activated_users\n",
    "\n",
    "    return active_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On souhaite comparer les différentes méthodes sur la dffusion avec Independant cascade. On fait des tests avec la taille de seed_users qui varie\n",
    "\n",
    "K_values = [5, 10, 20, 40]\n",
    "results_df = pd.DataFrame(index=K_values, columns=methodes)\n",
    "\n",
    "num_simulations = 10\n",
    "\n",
    "for centrality_measure, centrality_values in methodes.items():\n",
    "    sorted_users = sorted(centrality_values, key=centrality_values.get, reverse=True)\n",
    "    for K in K_values:\n",
    "        top_K_users = sorted_users[:K]\n",
    "        total_activated_users = 0\n",
    "        for _ in range(num_simulations):\n",
    "            activated_users = independent_cascade_simulation(G, top_K_users)\n",
    "            total_activated_users += len(activated_users)\n",
    "\n",
    "        average_activated_users = total_activated_users / num_simulations\n",
    "        results_df.at[K, centrality_measure] = average_activated_users\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Influence Maximization\n",
    "\n",
    "\n",
    "def influence_maximization_cascade(G, K, num_simulations=1):\n",
    "    seed_users = []\n",
    "\n",
    "    for _ in range(K):\n",
    "        best_user = -1\n",
    "        best_increase = 0\n",
    "\n",
    "        for user in G.nodes:\n",
    "            if user not in seed_users:\n",
    "                temp_seed_users = seed_users.copy()\n",
    "                temp_seed_users.append(user)\n",
    "\n",
    "                total_activated_users = 0\n",
    "                for _ in range(num_simulations):\n",
    "                    activated_users = independent_cascade_simulation(G, temp_seed_users)\n",
    "                    total_activated_users += len(activated_users)\n",
    "\n",
    "                average_activated_users = total_activated_users / num_simulations\n",
    "                increase = average_activated_users - len(seed_users)\n",
    "\n",
    "                if increase > best_increase:\n",
    "                    best_user = user\n",
    "                    best_increase = increase\n",
    "\n",
    "        seed_users.append(best_user)\n",
    "\n",
    "    return seed_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compléter result.df et ajouter la méthode influence maximization cascade\n",
    "\n",
    "results_df[\"influence_maximization\"] = None\n",
    "num_simulations = 1\n",
    "\n",
    "# Run the greedy influence maximization for different K values\n",
    "\n",
    "for K in K_values:\n",
    "    seed_users = influence_maximization_cascade(G, K)\n",
    "    print(K)\n",
    "    total_activated_users = 0\n",
    "    for _ in range(num_simulations):\n",
    "        activated_users = independent_cascade_simulation(G, seed_users)\n",
    "        total_activated_users += len(activated_users)\n",
    "\n",
    "    average_activated_users = total_activated_users / num_simulations\n",
    "    results_df.at[K, \"influence_maximization\"] = average_activated_users\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear treshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle Linear Threshold pour la diffusion des informations. On utilise un treshold aléatoire pour chaque arrete.\n",
    "\n",
    "\n",
    "def linear_threshold_simulation(G, seed_users):\n",
    "    active_users = seed_users.copy()\n",
    "    new_active_users = seed_users.copy()\n",
    "\n",
    "    while new_active_users:\n",
    "        new_active_users_temp = []\n",
    "        for user in new_active_users:\n",
    "            neighbors = G.neighbors(user)\n",
    "            for neighbor in neighbors:\n",
    "                if neighbor not in active_users:\n",
    "                    active_neighbors = [\n",
    "                        n for n in G.neighbors(neighbor) if n in active_users\n",
    "                    ]\n",
    "                    threshold = (\n",
    "                        random.random()\n",
    "                    )  # Generate a random threshold for each iteration\n",
    "                    if len(active_neighbors) / G.degree(neighbor) >= threshold:\n",
    "                        new_active_users_temp.append(neighbor)\n",
    "                        active_users.append(neighbor)\n",
    "        new_active_users = new_active_users_temp\n",
    "\n",
    "    return active_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On souhaite comparer les différentes méthodes sur la dffusion avec Linear treshold. On fait des tests avec la taille de seed_users qui varie\n",
    "\n",
    "K_values = [5, 10, 20, 40, 100, 500, 1000]\n",
    "results_df_treshold = pd.DataFrame(index=K_values, columns=methodes)\n",
    "\n",
    "num_simulations = 10\n",
    "\n",
    "for centrality_measure, centrality_values in methodes.items():\n",
    "    sorted_users = sorted(centrality_values, key=centrality_values.get, reverse=True)\n",
    "    for K in K_values:\n",
    "        top_K_users = sorted_users[:K]\n",
    "        total_activated_users = 0\n",
    "        for _ in range(num_simulations):\n",
    "            activated_users = linear_threshold_simulation(G, top_K_users)\n",
    "            total_activated_users += len(activated_users)\n",
    "\n",
    "        average_activated_users = total_activated_users / num_simulations\n",
    "        results_df_treshold.at[K, centrality_measure] = average_activated_users\n",
    "\n",
    "results_df_treshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Influence Maximization\n",
    "\n",
    "\n",
    "def influence_maximization_threshold(G, k, num_simulations=100):\n",
    "    seed_users = []\n",
    "\n",
    "    for _ in range(k):\n",
    "        best_user = None\n",
    "        best_influence = 0\n",
    "\n",
    "        for user in G.nodes():\n",
    "            if user not in seed_users:\n",
    "                temp_seed_users = seed_users.copy()\n",
    "                temp_seed_users.append(user)\n",
    "\n",
    "                total_influence = 0\n",
    "                for _ in range(num_simulations):\n",
    "                    active_users = linear_threshold_simulation(G, temp_seed_users)\n",
    "                    total_influence += len(active_users)\n",
    "\n",
    "                average_influence = total_influence / num_simulations\n",
    "\n",
    "                if average_influence > best_influence:\n",
    "                    best_user = user\n",
    "                    best_influence = average_influence\n",
    "\n",
    "        seed_users.append(best_user)\n",
    "\n",
    "    return seed_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compléter result.df_treshold et ajouter la méthode influence maximization treshold\n",
    "\n",
    "results_df_treshold[\"influence_maximization\"] = None\n",
    "num_simulations = 5\n",
    "\n",
    "# Run the greedy influence maximization for different K values\n",
    "\n",
    "for K in K_values:\n",
    "    seed_users = influence_maximization_threshold(G, K)\n",
    "\n",
    "    total_activated_users = 0\n",
    "    for _ in range(num_simulations):\n",
    "        activated_users = linear_threshold_simulation(G, seed_users)\n",
    "        total_activated_users += len(activated_users)\n",
    "\n",
    "    average_activated_users = total_activated_users / num_simulations\n",
    "    results_df.at[K, \"influence_maximization\"] = average_activated_users\n",
    "\n",
    "results_df_treshold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
