{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des différents packages\n",
    "\n",
    "import networkx as nx\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List\n",
    "import matplotlib\n",
    "import json\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrer les caractéristiques dans stats_twitter.json\n",
    "\n",
    "def save_stats(stats):\n",
    "    with open(\"stats_twitter.json\", \"w\") as f:\n",
    "        json.dump(stats, f, sort_keys=True, indent=4)\n",
    "\n",
    "def load_stats() -> Dict:\n",
    "    with open(\"stats_twitter.json\", \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def print_stats(stats):\n",
    "    print(json.dumps(stats, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modification sur le style du graph\n",
    "\n",
    "def highlight_nodes(graph: nx.Graph, nodes: List = []):\n",
    "    reset_colors(graph)\n",
    "    for node in nodes:\n",
    "        graph.nodes[node][\"viz\"][\"color\"] = {'r': 255, 'g': 0, 'b': 0, 'a': 1}\n",
    "\n",
    "def highlight_nodes_importance(graph: nx.Graph, node_importance : Dict = {}):\n",
    "    reset_colors(graph)\n",
    "    min = np.min(list(node_importance.values()))\n",
    "    max = np.max(list(node_importance.values()))\n",
    "    norm = matplotlib.colors.Normalize(vmin=min, vmax=max)\n",
    "    for node, importance in node_importance.items():\n",
    "        g = int(255 * norm(importance))\n",
    "        r = 255 - g\n",
    "        graph.nodes[node][\"viz\"][\"color\"] = {'r': r, 'g': g, 'b': 0, 'a': 1}\n",
    "\n",
    "def highlight_nodes_communities(graph: nx.Graph, communities: List[List] = []):\n",
    "    reset_colors(graph)\n",
    "    cmap: LinearSegmentedColormap = plt.get_cmap('hsv')\n",
    "    for i, community in enumerate(communities):\n",
    "        (r, g, b, a) = cmap(i / len(communities))\n",
    "        r, g, b, a = int(r * 255), int(g * 255), int(b * 255), a\n",
    "        for node in community:\n",
    "            graph.nodes[node][\"viz\"][\"color\"] = {'r': r, 'g': g, 'b': b, 'a': a}\n",
    "\n",
    "\n",
    "def reset_colors(graph: nx.Graph):\n",
    "    for node in graph:\n",
    "        if(not \"viz\" in graph.nodes[node]):\n",
    "            graph.nodes[node][\"viz\"] = {}\n",
    "        graph.nodes[node][\"viz\"][\"color\"] = {'r': 173, 'g': 216, 'b': 230, 'a': 1}\n",
    "\n",
    "\n",
    "def circular_layout(graph: nx.Graph):\n",
    "    pos = nx.circular_layout(graph, scale=4*graph.number_of_nodes())\n",
    "    for node, position in pos.items():\n",
    "        if(not \"viz\" in graph.nodes[node]):\n",
    "            graph.nodes[node][\"viz\"] = {}\n",
    "        graph.nodes[node][\"viz\"][\"position\"] = {\"x\": position[0], \"y\": position[1], \"z\": 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs_sampling(G, start_node, num_nodes_to_sample):\n",
    "    visited = set()\n",
    "    queue = [start_node]\n",
    "\n",
    "    while queue and len(visited) < num_nodes_to_sample:\n",
    "        node = queue.pop(0)\n",
    "        if node not in visited:\n",
    "            visited.add(node)\n",
    "            neighbors = list(G.neighbors(node))\n",
    "            random.shuffle(neighbors)\n",
    "            queue.extend(neighbors)\n",
    "\n",
    "    subgraph = G.subgraph(visited)\n",
    "    return subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'datasets/twitter/twitter_combined.txt'\n",
    "STATS = load_stats()\n",
    "\n",
    "G = nx.read_edgelist(path=DATASET_PATH, create_using=nx.DiGraph(), nodetype=int)\n",
    "\n",
    "\n",
    "\n",
    "centrality = nx.degree_centrality(G)\n",
    "# Trouvez le noeud avec la plus grande centralité de degré\n",
    "max_degree_node = max(centrality, key=centrality.get)\n",
    "# On garde une partie du graph en partant de ce noeud\n",
    "G = bfs_sampling(G, max_degree_node,3000)\n",
    "\n",
    "\n",
    "circular_layout(G)\n",
    "reset_colors(G)\n",
    "\n",
    "nx.write_gexf(G, \"graphs/twitter/graph.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATS[\"number_of_nodes\"] =  G.number_of_nodes()\n",
    "save_stats(STATS)\n",
    "\n",
    "STATS[\"number_of_edges\"] =  G.number_of_edges()\n",
    "save_stats(STATS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étude de la centralité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la centralité de degré pour les nœuds échantillonnés\n",
    "\n",
    "STATS[\"degree_centrality\"] = nx.degree_centrality(G)\n",
    "save_stats(STATS)\n",
    "\n",
    "highlight_nodes_importance(G, STATS[\"degree_centrality\"])\n",
    "nx.write_gexf(G, \"graphs/twitter/degree_centrality.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la centralité avec pagerank\n",
    "\n",
    "STATS[\"pagerank\"] = nx.pagerank(G)\n",
    "save_stats(STATS)\n",
    "\n",
    "highlight_nodes_importance(G, STATS[\"pagerank\"])\n",
    "nx.write_gexf(G, \"graphs/twitter/pagerank.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul la centralité avec closeness_centrality\n",
    "\n",
    "STATS[\"closeness_centrality\"] = nx.closeness_centrality(G)\n",
    "save_stats(STATS)\n",
    "highlight_nodes_importance(G, STATS[\"closeness_centrality\"])\n",
    "nx.write_gexf(G, \"graphs/twitter/closeness_centrality.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul la centralité avec betweenness_centrality\n",
    "\n",
    "STATS[\"betweenness_centrality\"] = nx.betweenness_centrality(G)\n",
    "save_stats(STATS)\n",
    "highlight_nodes_importance(G, STATS[\"betweenness_centrality\"])\n",
    "nx.write_gexf(G, \"graphs/twitter/betweenness_centrality.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "methodes = {\"degree_centrality\":STATS[\"degree_centrality\"],\"closeness_centrality\":STATS[\"closeness_centrality\"]\n",
    "            ,\"betweenness_centrality\":STATS[\"betweenness_centrality\"],\"pagerank\":STATS[\"pagerank\"]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independant cascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle Independent_cascade pour la diffusion des informations. On prend w(u,v) = 1/dv-\n",
    "\n",
    "def independent_cascade_simulation(G, seed_users):\n",
    "    active_users = seed_users.copy()\n",
    "    new_active_users = seed_users.copy()\n",
    "\n",
    "    while new_active_users:\n",
    "        newly_activated_users = []\n",
    "\n",
    "        for active_user in new_active_users:\n",
    "            for neighbor in G.neighbors(active_user):\n",
    "                if neighbor not in active_users:\n",
    "                    indegree = G.in_degree(neighbor)\n",
    "                    p = 1 / indegree\n",
    "\n",
    "                    if random.random() <= p:\n",
    "                        newly_activated_users.append(neighbor)\n",
    "                        active_users.append(neighbor)\n",
    "\n",
    "        new_active_users = newly_activated_users\n",
    "\n",
    "    return active_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>degree_centrality</th>\n",
       "      <th>closeness_centrality</th>\n",
       "      <th>betweenness_centrality</th>\n",
       "      <th>eigenvector_centrality</th>\n",
       "      <th>pagerank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>715.2</td>\n",
       "      <td>657.2</td>\n",
       "      <td>824.5</td>\n",
       "      <td>138.5</td>\n",
       "      <td>657.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>771.2</td>\n",
       "      <td>664.1</td>\n",
       "      <td>845.9</td>\n",
       "      <td>326.5</td>\n",
       "      <td>608.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>909.6</td>\n",
       "      <td>739.7</td>\n",
       "      <td>932.0</td>\n",
       "      <td>343.1</td>\n",
       "      <td>708.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1035.9</td>\n",
       "      <td>850.7</td>\n",
       "      <td>1137.5</td>\n",
       "      <td>402.3</td>\n",
       "      <td>763.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   degree_centrality closeness_centrality betweenness_centrality  \\\n",
       "5              715.2                657.2                  824.5   \n",
       "10             771.2                664.1                  845.9   \n",
       "20             909.6                739.7                  932.0   \n",
       "40            1035.9                850.7                 1137.5   \n",
       "\n",
       "   eigenvector_centrality pagerank  \n",
       "5                   138.5    657.3  \n",
       "10                  326.5    608.4  \n",
       "20                  343.1    708.7  \n",
       "40                  402.3    763.6  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On souhaite comparer les différentes méthodes sur la dffusion avec Independant cascade. On fait des tests avec la taille de seed_users qui varie\n",
    "\n",
    "K_values = [5,10,20,40]\n",
    "results_df = pd.DataFrame(index=K_values, columns=methodes)\n",
    "\n",
    "num_simulations = 10\n",
    "\n",
    "for centrality_measure, centrality_values in methodes.items():\n",
    "    sorted_users = sorted(centrality_values, key=centrality_values.get, reverse=True)\n",
    "    for K in K_values:\n",
    "        top_K_users = sorted_users[:K]\n",
    "        total_activated_users = 0\n",
    "        for _ in range(num_simulations):\n",
    "            activated_users = independent_cascade_simulation(G, top_K_users)\n",
    "            total_activated_users += len(activated_users)\n",
    "\n",
    "        average_activated_users = total_activated_users / num_simulations\n",
    "        results_df.at[K, centrality_measure] = average_activated_users\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Influence Maximization\n",
    "\n",
    "def influence_maximization_cascade(G, K,num_simulations=1):\n",
    "    seed_users = []\n",
    "\n",
    "    for _ in range(K):\n",
    "        best_user = -1\n",
    "        best_increase = 0\n",
    "\n",
    "        for user in G.nodes:\n",
    "            if user not in seed_users:\n",
    "                temp_seed_users = seed_users.copy()\n",
    "                temp_seed_users.append(user)\n",
    "\n",
    "                total_activated_users = 0\n",
    "                for _ in range(num_simulations):\n",
    "                    activated_users = independent_cascade_simulation(G, temp_seed_users)\n",
    "                    total_activated_users += len(activated_users)\n",
    "\n",
    "                average_activated_users = total_activated_users / num_simulations\n",
    "                increase = average_activated_users - len(seed_users)\n",
    "\n",
    "                if increase > best_increase:\n",
    "                    best_user = user\n",
    "                    best_increase = increase\n",
    "\n",
    "        seed_users.append(best_user)\n",
    "\n",
    "    return seed_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compléter result.df et ajouter la méthode influence maximization cascade\n",
    "\n",
    "results_df['influence_maximization'] = None\n",
    "num_simulations = 1\n",
    "\n",
    "# Run the greedy influence maximization for different K values\n",
    "\n",
    "for K in K_values:\n",
    "    seed_users = influence_maximization_cascade(G, K)\n",
    "    print(K)\n",
    "    total_activated_users = 0\n",
    "    for _ in range(num_simulations):\n",
    "        activated_users = independent_cascade_simulation(G, seed_users)\n",
    "        total_activated_users += len(activated_users)\n",
    "\n",
    "    average_activated_users = total_activated_users / num_simulations\n",
    "    results_df.at[K, 'influence_maximization'] = average_activated_users\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear treshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle Linear Threshold pour la diffusion des informations. On utilise un treshold aléatoire pour chaque arrete.\n",
    "\n",
    "def linear_threshold_simulation(G, seed_users):\n",
    "    active_users = seed_users.copy()\n",
    "    new_active_users = seed_users.copy()\n",
    "\n",
    "    while new_active_users:\n",
    "        new_active_users_temp = []\n",
    "        for user in new_active_users:\n",
    "            neighbors = G.neighbors(user)\n",
    "            for neighbor in neighbors:\n",
    "                if neighbor not in active_users:\n",
    "                    active_neighbors = [n for n in G.neighbors(neighbor) if n in active_users]\n",
    "                    threshold = random.random()  # Generate a random threshold for each iteration\n",
    "                    if len(active_neighbors) / G.degree(neighbor) >= threshold:\n",
    "                        new_active_users_temp.append(neighbor)\n",
    "                        active_users.append(neighbor)\n",
    "        new_active_users = new_active_users_temp\n",
    "\n",
    "    return active_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On souhaite comparer les différentes méthodes sur la dffusion avec Linear treshold. On fait des tests avec la taille de seed_users qui varie\n",
    "\n",
    "K_values = [5,10,20,40,100,500,1000]\n",
    "results_df_treshold = pd.DataFrame(index=K_values, columns=methodes)\n",
    "\n",
    "num_simulations = 10\n",
    "\n",
    "for centrality_measure, centrality_values in methodes.items():\n",
    "    sorted_users = sorted(centrality_values, key=centrality_values.get, reverse=True)\n",
    "    for K in K_values:\n",
    "        top_K_users = sorted_users[:K]\n",
    "        total_activated_users = 0\n",
    "        for _ in range(num_simulations):\n",
    "            activated_users = linear_threshold_simulation(G, top_K_users)\n",
    "            total_activated_users += len(activated_users)\n",
    "\n",
    "        average_activated_users = total_activated_users / num_simulations\n",
    "        results_df_treshold.at[K, centrality_measure] = average_activated_users\n",
    "\n",
    "results_df_treshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Influence Maximization\n",
    "\n",
    "def influence_maximization_threshold(G, k, num_simulations=100):\n",
    "    seed_users = []\n",
    "\n",
    "    for _ in range(k):\n",
    "        best_user = None\n",
    "        best_influence = 0\n",
    "\n",
    "        for user in G.nodes():\n",
    "            if user not in seed_users:\n",
    "                temp_seed_users = seed_users.copy()\n",
    "                temp_seed_users.append(user)\n",
    "\n",
    "                total_influence = 0\n",
    "                for _ in range(num_simulations):\n",
    "                    active_users = linear_threshold_simulation(G, temp_seed_users)\n",
    "                    total_influence += len(active_users)\n",
    "\n",
    "                average_influence = total_influence / num_simulations\n",
    "\n",
    "                if average_influence > best_influence:\n",
    "                    best_user = user\n",
    "                    best_influence = average_influence\n",
    "\n",
    "        seed_users.append(best_user)\n",
    "\n",
    "    return seed_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compléter result.df_treshold et ajouter la méthode influence maximization treshold\n",
    "\n",
    "results_df_treshold['influence_maximization'] = None\n",
    "num_simulations = 5\n",
    "\n",
    "# Run the greedy influence maximization for different K values\n",
    "\n",
    "for K in K_values:\n",
    "    seed_users = influence_maximization_threshold(G, K)\n",
    "    \n",
    "    total_activated_users = 0\n",
    "    for _ in range(num_simulations):\n",
    "        activated_users = linear_threshold_simulation(G, seed_users)\n",
    "        total_activated_users += len(activated_users)\n",
    "\n",
    "    average_activated_users = total_activated_users / num_simulations\n",
    "    results_df.at[K, 'influence_maximization'] = average_activated_users\n",
    "\n",
    "results_df_treshold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
